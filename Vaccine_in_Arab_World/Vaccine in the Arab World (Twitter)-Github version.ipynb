{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10dd3cdf",
   "metadata": {},
   "source": [
    "## How do sentiments towards COVID-19 vaccine compare before and after the emergence of the Omicron variant in the Arab world?\n",
    "### A Twitter comparative sentiment analysis of the pre-Omicron and post-Omicron phases by Arabic users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fe3c7f9",
   "metadata": {},
   "source": [
    "### Dependency - The analysis leverages CAMeL Tools\n",
    "### CAMeL Tools is a suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi.\n",
    "#### pip3 install camel_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10952dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "import os\n",
    "os.environ[\"CAMELTOOLS_DATA\"] = \"~/.camel_tools\"\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "text = 'ﷺ'\n",
    "\n",
    "sentence = \"sentence from tweet\"\n",
    "\n",
    "sent_norm = normalize_unicode(sentence)\n",
    "\n",
    "\n",
    "# Normalize alef variants to 'ا'\n",
    "sent_norm = normalize_alef_ar(sentence)\n",
    "\n",
    "# Normalize alef maksura 'ى' to yeh 'ي'\n",
    "sent_norm = normalize_alef_maksura_ar(sent_norm)\n",
    "\n",
    "# Normalize teh marbuta 'ة' to heh 'ه'\n",
    "sent_norm = normalize_teh_marbuta_ar(sent_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397017d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the morphological database.\n",
    "# The MorphologyDB database is used for analyzing modern Standard Arabic. \n",
    "db = MorphologyDB.builtin_db()\n",
    "\n",
    "analyzer = Analyzer(db)\n",
    "\n",
    "analyses = analyzer.analyze('موظف')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655bbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "# The tokenizer expects pre-tokenized text\n",
    "sentence = simple_word_tokenize(\"sentence from tweet\")\n",
    "\n",
    "# Load a pretrained disambiguator to use with a tokenizer\n",
    "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "# Without providing additional arguments, the tokenizer will output undiacritized\n",
    "# morphological tokens for each input word delimited by an underscore.\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# By specifying `split=True`, the morphological tokens are output as seperate\n",
    "# strings.\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# We can output diacritized tokens by setting `diac=True`\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
    "tokens = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "204ab457",
   "metadata": {},
   "source": [
    "## Import raw data\n",
    "### Please see Data_by_day notebook for the method by which the pre and post files were created\n",
    "#### Tweets using the keywords “لقاحات” or “تطعيم” or “لقاح” or “تطعيمات”  - ”vaccines”, “inoculation”, ”vaccine”, “inoculations” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1_tweets = pd.read_csv('put_your_file_location_for_pre_omicron.csv', index_col=0)\n",
    "df1_tweets = df1_tweets[ df1_tweets['text'].str.startswith('RT') == False ]\n",
    "\n",
    "df2_tweets = pd.read_csv('put_your_file_location_for_post_omicron.csv', index_col=0)\n",
    "df2_tweets = df2_tweets[ df2_tweets['text'].str.startswith('RT') == False ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed94dbc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74e7901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Arabic stopwords from this file\n",
    "with open('ar_stopwords.txt', 'r') as file:\n",
    "    stopwords = file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "109b437c",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e086503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "#remove stopwords and all characters that are not arabic letters or # numbers and lemmatize the words\n",
    "def preprocess_ar(text):\n",
    "    processedText = []\n",
    "    \n",
    "    my_stp = stp.stopwords_list() | { u'كورونا', u'ان' }\n",
    "\n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    st = ISRIStemmer()\n",
    "\n",
    "    for t in text:\n",
    "        t = ''.join(c for c in t if ud.category(c) == 'Lo' or ud.category(c) == 'Nd' or c == ' ')\n",
    "   \n",
    "        commentwords = ''\n",
    "        for word in t.split():\n",
    "            # Checking if the word is a stopword.\n",
    "\n",
    "            if word not in my_stp:\n",
    "                if len(word)>1:\n",
    "                    # Lemmatizing the word.\n",
    "                    word = st.suf32(word)\n",
    "                    commentwords += (word+' ')\n",
    "        processedText.append(normalize_unicode(commentwords))\n",
    "    \n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import unicodedata as ud\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "\n",
    "df1_tweets = pd.read_csv('put_your_file_location_for_tweet_data.csv', index_col=0)\n",
    "\n",
    "df1_tweets = df1_tweets[ df1_tweets['text'].str.startswith('RT') == False ]\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "processedtext_ar1 = preprocess_ar( df1_tweets.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('put_your_file_location_for_pre_omicron_df.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(processedtext_ar1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40adc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_tweets = pd.read_csv('put_your_file_location_for_data_by_day_post_omicron.csv', index_col=0)\n",
    "\n",
    "df2_tweets = df2_tweets[ df2_tweets['text'].str.startswith('RT') == False ]\n",
    "\n",
    "t = time.time()\n",
    "processedtext_ar2 = preprocess_ar(df2_tweets.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdd39fb3",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c71e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ar_wordcloud import ArabicWordCloud\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d82aa574",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0029a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Arabic stopwords to the natural language toolkit\n",
    "stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "stopwords.extend(['Arabic stopwords'])\n",
    "stopwords.append( stopwords)\n",
    "\n",
    "#plot Arabic wordcloud\n",
    "awc = ArabicWordCloud(width=2000,height=1600,max_font_size=400,max_words=10000,collocations=False, background_color='skyblue', colormap=\"Purples\")\n",
    "plt.figure(figsize=(16,16))\n",
    "#save figure\n",
    "wc_ar = awc.from_text(u''.join(processedtext_ar1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc_ar)\n",
    "plt.savefig(\"WC1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ddbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "awc = ArabicWordCloud(width=2000,height=1600,max_font_size=400,max_words=10000,collocations=False, background_color='skyblue', colormap=\"Purples\")\n",
    "plt.figure(figsize=(16,16))\n",
    "wc_ar = awc.from_text(u''.join(processedtext_ar2))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc_ar)\n",
    "plt.savefig(\"WC2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7245e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify dialect by city\n",
    "from camel_tools.dialectid import DialectIdentifier\n",
    "\n",
    "did = DialectIdentifier.pretrained()\n",
    "\n",
    "sentences = [\n",
    "    'Sentence from tweet'\n",
    "]\n",
    "\n",
    "predictions = did.predict(sentences, 'city')\n",
    "\n",
    "predictions = did.predict(sentences, 'country')\n",
    "\n",
    "predictions = did.predict(sentences, 'region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da702197",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = []\n",
    "# omit bad records using the following example - will require iterative runs.\n",
    "nums = [10009, 13105, 14064, 16699, 16787, 1691, 16915, 17610, 17611 ]\n",
    "for i in range(16914,len(processedtext_ar1)):\n",
    "    if i in nums:\n",
    "        continue\n",
    "    text = processedtext_ar1[ i ]\n",
    "    sen1.append([sa.predict(text)[0], did.predict(text, 'city')[0].top, did.predict(text, 'country')[0].top, did.predict(text, 'region')[0].top])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c62869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect sentiments by city\n",
    "sentiments1 = pd.DataFrame( sen1, columns = ['sentiment', 'city', 'country', 'region' ] )\n",
    "sentiments1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_all = pd.concat( [ sentiments1, pd.read_csv('file_location_sentiments.csv') ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "from camel_tools.dialectid import DialectIdentifier\n",
    "from random import sample\n",
    "did = DialectIdentifier.pretrained()\n",
    "sa = SentimentAnalyzer.pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67496bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2 = []\n",
    "# omit bad records using the following example - will require iterative runs.\n",
    "nums = [ 609, 2326, 2327, 2955, 2956, 4523, 4524, 4525,7187,7858, 8795, \n",
    "         10871, 12531, 12532, 16521, 18575, 24564, 33337, 36830 ]\n",
    "for i in range(36827,len(processedtext_ar2)):\n",
    "    if i in nums:\n",
    "        continue\n",
    "    text = processedtext_ar2[ i ]\n",
    "    sen2.append([sa.predict(text)[0], did.predict(text, 'city')[0].top, did.predict(text, 'country')[0].top, did.predict(text, 'region')[0].top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments2 = pd.DataFrame( sen2, columns = ['sentiment', 'city', 'country', 'region' ] )\n",
    "sentiments2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962477d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_all = pd.concat( [  sentiments2,\n",
    "                        pd.read_csv('file_location_sentiments.csv') ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe50c62",
   "metadata": {},
   "source": [
    "### Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a0b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a matrix of sentiments by city\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pprint as pp\n",
    "titles = ['sentiment', 'city', 'country', 'region'] \n",
    "sentiment_type = ['positive', 'negative', 'neutral'] \n",
    "\n",
    "def build_matrix(sentiments):\n",
    "    matrix = {}\n",
    "\n",
    "    for row in sentiments.values.astype(str).tolist():\n",
    "        if not matrix.get(row[0]):\n",
    "            matrix[ row[0] ] = {}\n",
    "\n",
    "        if not matrix[ row[0] ].get( row[1] ) :\n",
    "            matrix[ row[0] ] [ row[1] ] = {}\n",
    "            matrix[ row[0] ] [ row[1] ] = 1      \n",
    "        else:\n",
    "            matrix[ row[0] ] [ row[1] ] += 1      \n",
    "            \n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f46f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame using pandas to compare the occurrence of each word across all three sentiments\n",
    "df1 = pd.DataFrame( build_matrix( sentiments1 ))\n",
    "\n",
    "df1\n",
    "ax= df1.plot.bar(color = ('#FF8C00', '#228B22','#1E90FF' ), figsize=(20, 10))\n",
    "ax.set_xlabel('Cities', fontsize=30)\n",
    "ax.set_ylabel('Frequency', fontsize=30)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot sentiments frequency by city\n",
    "df2 = pd.DataFrame( build_matrix( sentiments2 ))\n",
    "\n",
    "df2\n",
    "ax= df2.plot.bar(color = ('#1E90FF','#228B22','#FF8C00'), figsize=(20, 10))\n",
    "ax.set_xlabel('Cities', fontsize=30)\n",
    "ax.set_ylabel('Frequency', fontsize=30)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.legend(fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the most common words found in tweets \n",
    "counts = dict()\n",
    "for i in range(0,len(processedtext_ar1)):\n",
    "    for word in processedtext_ar1[i].split(' '):\n",
    "        if  not counts.get(''.join(reversed(word))):\n",
    "            counts[ u''.join(reversed(word)) ] = 1\n",
    "        else:\n",
    "            counts[ u''.join(reversed(word)) ] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the most common words found in tweets in Arabic for pre-Omicron tweets\n",
    "new_vocab1 = Counter( counts )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "clean_tweets = pd.DataFrame(new_vocab1.most_common(20), columns=['words', 'count'])\n",
    "clean_tweets.sort_values(by='count').plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"brown\")\n",
    "\n",
    "ax.set_title(\"Common Words Found in Tweets (Including All Words)\")\n",
    "plt.show()\n",
    "print(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most Common Words translated to English for pre-Omicon tweets\n",
    "my_dict1= {u'vaccine':15031, u'vaccination': 3217, u'inoculation':2102, u'god':1520, u'dose': 1448, u'pfizer': 1389, u'health': 1168, u'me':1108, u'doses': 1106, u'protected': 935, u'that':931, u'the dose': 913, u'a dose':897   }\n",
    "plt.barh(*zip(*my_dict1.items()))\n",
    "plt.savefig(\"most_common_words1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count and plot the most common words found in tweets for post-Omicron tweets\n",
    "counts = dict()\n",
    "for i in range(0,len(processedtext_ar2)):\n",
    "    for word in processedtext_ar2[i].split(' ') :\n",
    "        print(word)\n",
    "        if  not counts.get(''.join(reversed(word))):\n",
    "            counts[ ''.join(reversed(word)) ] = 1\n",
    "        else:\n",
    "            counts[ ''.join(reversed(word)) ] += 1\n",
    "\n",
    "new_vocab2 = Counter( counts )\n",
    "         \n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "clean_tweets = pd.DataFrame(new_vocab2.most_common(20), columns=['words', 'count'])\n",
    "clean_tweets.sort_values(by='count').plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"brown\")\n",
    "\n",
    "ax.set_title(\"Common Words Found in Tweets (Including All Words)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90237114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most Common Words translated to English for post-Omicon tweets\n",
    "my_dict2= {u'vaccine':39470, u'presence': 10241, u'dose':7687, u'the dose':7610, u'surrender': 5067, u'drop': 4577, u'vaccination': 4543, u'accomplishment':4532, u' a dose': 4505, u'uptake':4169, u'the second': 4035, u'first':3484   }\n",
    "\n",
    "plt.barh(*zip(*my_dict2.items()))\n",
    "plt.savefig(\"most_common_words2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5294d70",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f776c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot entiment analysis for pre-Omicron tweets\n",
    "sentiment_counts = (df1['positive'].sum(), df1['negative'].sum(), df1['neutral'].sum())\n",
    "labels=[\"Positive\",\"Negative\", \"Neutral\"]\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "ax1.pie(sentiment_counts, labels=labels, autopct='%1.1f%%', shadow=False, startangle=90 )\n",
    "ax1.axis('equal')  \n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment Distribution for Pre-Omicron Dataset')\n",
    "plt.savefig(\"sentiment1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot entiment analysis for post-Omicron tweets\n",
    "sentiment_counts = (df2['positive'].sum(), df2['negative'].sum(), df2['neutral'].sum())\n",
    "labels=[\"Positive\",\"Negative\", \"Neutral\"]\n",
    "#colors=[\"#ff9999\",\"#99ff99\"]\n",
    "#explode = (0, 0.1)\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "ax1.pie(sentiment_counts, labels=labels, autopct='%1.1f%%', shadow=False, startangle=90)\n",
    "ax1.axis('equal')  \n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment Distribution for Post-Omicron Dataset')\n",
    "plt.savefig(\"sentiment2.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
